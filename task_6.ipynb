{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZwNusbLVcbQ"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тестовое множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача №1"
      ],
      "metadata": {
        "id": "Qw0XlG1q2fqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YpCCStYf2hky"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('news_train.txt') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "El6cnd1Jmsjj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('news_test.txt') as f:\n",
        "    lines_2 = f.readlines()"
      ],
      "metadata": {
        "id": "HiYRXlMrws0l"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "RDqSDkXxLcOk",
        "outputId": "6acccdb7-189d-489a-c1be-3bd8bff5301d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sport\\tОвечкин пожертвовал детской хоккейной школе автомобиль\\tНападающий «Вашингтон Кэпиталз» Александр Овечкин передал детской хоккейной школе автомобиль, полученный им после окончания Матча всех звезд Национальной хоккейной лиги (НХЛ). Об этом сообщается на официальном сайте лиги.Автомобиль Honda Accord был подарен хоккеисту по решению спонсоров мероприятия. Игрок НХЛ пожертвовал машину спортивной школе Nova Cool Cats Special Hockey Inc., которая расположена в штате Вирджиния.Овечкин общается с 10-летней девочкой Анной Шоб с синдромом Дауна, которая занимается в этой школе и является поклонницей спортсмена. В сентябре форвард пообедал вместе с юной хоккеисткой в японском ресторане.Матч всех звезд НХЛ в Коламбусе (штат Огайо) завершился победой команды «Джонатана Тэйвза» над командой «Ника Фолиньо» со счетом 17:12. Овечкин выступал за проигравший коллектив. Россиянин отметился тремя результативными передачами.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import wordpunct_tokenize, word_tokenize, TweetTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "tweet_tokenize = TweetTokenizer()\n",
        "\n",
        "tokenize_list = []\n",
        "for sent in lines:\n",
        "    tokenize_list.append(word_tokenize(sent))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJIoBO3mNTS",
        "outputId": "a667d7a1-23bd-4351-96b4-0205fab8307a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "Sw6Ga-vzy71v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"russian\")"
      ],
      "metadata": {
        "id": "I6EC4PHcy8aU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_list =[]\n",
        "for i in range(len(tokenize_list)):\n",
        "  temp = []\n",
        "  for k in range(1,len(tokenize_list[i])):\n",
        "    temp.append(stemmer.stem(tokenize_list[i][k]))\n",
        "  stem_list.append(temp)"
      ],
      "metadata": {
        "id": "SGKCmO7Vz_Zd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "6uvJRf65KV0-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(stem_list)):\n",
        "  stem_list[i]= ' '.join(stem_list[i])"
      ],
      "metadata": {
        "id": "huPWrx-GQudk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(stem_list_2)):\n",
        "  stem_list_2[i]= ' '.join(stem_list_2[i])"
      ],
      "metadata": {
        "id": "CY5MlyMLxs3z"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "U-nYbiSJTYS8",
        "outputId": "76735f3f-9fe8-4ef6-a603-74540bfb65e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'овечкин пожертвова детск хоккейн школ автомобил напада « вашингтон кэпиталз » александр овечкин переда детск хоккейн школ автомобил , получен им посл окончан матч всех звезд национальн хоккейн лиг ( нхл ) . об эт сообща на официальн сайт лиги.автомобил Hond Accord был подар хоккеист по решен спонсор мероприят . игрок нхл пожертвова машин спортивн школ Nov Cool Cats Specia Hocke Inc. , котор располож в штат вирджиния.овечкин обща с 10-летн девочк ан шоб с синдром даун , котор занима в эт школ и явля поклонниц спортсм . в сентябр форвард пообеда вмест с юн хоккеистк в японск ресторане.матч всех звезд нхл в коламбус ( штат огай ) заверш побед команд « джоната тэйвз » над команд « ник фолин » со счет 17:12 . овечкин выступа за проигра коллект . россиянин отмет трем результативн передач .'"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem_list_2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "hqKCz0xjx6U8",
        "outputId": "2556393e-cb76-4aa5-9c74-603e9493cede"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"жительниц яма побед в перв песен конкурс « нов звезд » жительниц ямало-ненецк автономн округ ел лаптандер побед в перв всероссийск песен конкурс « нов звезд » , сообщ « ленте.р » организатор событ . в качеств наград е доста статуэтк в форм звезд и денежн приз — один миллион рубл . по слов финалистк , вознагражден он планир переда в благотворительн фонд « подар жизн » .три дополнительн приз доста руслан ивакин из хакас , фолк-групп « ярил зно » из воронежск област и александр куулар из тыв . призер с помощ смс-голосован выбира зрител телекана « звезд » , котор транслирова конкурс . всег на финальн гала-концерт выступ 16 участников.ведущ шо стал актер серг безрук и аврор . на концерт прозвуча популярн песн воен лет : « журавл » , « довоен вальс » , « мо мил , есл б не был войн » , « цвет на дорог войн » , песн десантно-штурмов батальон из кинофильм « белорусск вокза » и другие. « на эт конкурс произошел прор — наступлен фольклор . настоя этник ! » , — отмет композитор макс дунаевск , тож приня участ в гала-концерте.всероссийск конкурс исполнител песн « нов звезд » проход при поддержк министерств оборон росс . ег иде принадлеж глав ведомств — серг шойг . « благодар эт проект мы услыша не ту `` синтетик ' , котор сейчас поголовн поет молодеж , а хорош голос и настоя музык . над , зрител по достоинств оцен тог певц , котор будет для стран `` нов звезд ' » , — прокомментирова шойг . мероприят планир провод ежегодн .\""
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________________________"
      ],
      "metadata": {
        "id": "2zOEEpMXHvgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m nltk.downloader stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nMCcXnZ9K4J",
        "outputId": "0789ad03-0460-4d0b-9d04-88f1a0b0390f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#--------#\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from string import punctuation\n",
        "\n",
        "#Create lemmatizer and stopwords list\n",
        "mystem = Mystem() \n",
        "russian_stopwords = stopwords.words(\"russian\")\n"
      ],
      "metadata": {
        "id": "t-fKTyri31BN"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl-gpXItKXZO",
        "outputId": "8223eefb-3c0a-4ed2-df49-254f74d141b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "Rj-jgNJ0KcfJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph.parse('избегали')[1].normal_form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4X1CZNKuLj3W",
        "outputId": "4e0e8533-9b05-4781-93f2-079b6fe9364b"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'избегать'"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "#--------#\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from string import punctuation\n",
        "russian_stopwords = stopwords.words(\"russian\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-UOsYag3sfl",
        "outputId": "6096c899-586b-4be6-b7fa-2dcbc86a0b8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    tweet_tokenize = TweetTokenizer()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "\n",
        "    #print(stem_list)\n",
        "   \n",
        "\n",
        "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
        "              and token != \" \" \\\n",
        "              and len(token)>=3 \\\n",
        "              and token.strip() not in punctuation \\\n",
        "              and token.isdigit()==False]\n",
        "\n",
        "    norm_list = []\n",
        "    for i in tokens:\n",
        "      norm_list.append(morph.parse(i)[0].normal_form)\n",
        "    tokens = norm_list\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "jx3D_yGg93sD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "______________________________________"
      ],
      "metadata": {
        "id": "-lZ8EMRyHpJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача №3"
      ],
      "metadata": {
        "id": "tzJP6QIk2kH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")\n",
        "#> 'сказать видеть кто-то наступать грабли разочаровывать натравлять'\n",
        "\n",
        "preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\")\n",
        "#> 'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HOpmjEAL4ImN",
        "outputId": "c2347dc3-f90c-479a-a840-aaed1c8f04b6"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text('Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был нат')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "A_nVMpqv-NV_",
        "outputId": "131e8ec9-b529-4816-8972-8951b57bb56d"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'сказать видеть кто-то наступить грабли разочаровать нат'"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "vectorizer = TfidfVectorizer(preprocessor= preprocess_text,ngram_range=(1, 3), max_df=0.5, max_features=1000)\n",
        "vectors = vectorizer.fit_transform(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5aVuIuAOHs7",
        "outputId": "c4f5b638-3743-44b7-b210-b4edb117aadb"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8min 56s, sys: 3.87 s, total: 9min\n",
            "Wall time: 9min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_2 = vectorizer.transform(lines_2)"
      ],
      "metadata": {
        "id": "IZKNRuVmzaHE"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nR3Fv_SqN3R",
        "outputId": "20f43eb6-049a-4f94-e857-0f3f7071e3b9"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 170468 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "7HGfiTUFsdQj"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_vectors = vectors.todense()\n",
        "dense_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iTNrFqTstJx",
        "outputId": "9c77c495-7678-4ae9-a213-769a3171e706"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense_vectors_2 = vectors_2.todense()\n",
        "dense_vectors_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymKrSnuLzzBn",
        "outputId": "9f55b175-2b3a-41a9-8fe8-f1482a52cd2c"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(v1, v2):\n",
        "    # v1, v2 (1 x dim)\n",
        "    return np.array(v1 @ v2.T / norm(v1) / norm(v2))[0][0]"
      ],
      "metadata": {
        "id": "aLt-aQ31s5Qe"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim(dense_vectors[0], dense_vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuR0fTVOtAcW",
        "outputId": "57c4c1b4-fd11-4752-bbcb-d331fc4117d4"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosines = []\n",
        "for i in range(10):\n",
        "    cosines.append(cosine_sim(dense_vectors[0], dense_vectors[i]))"
      ],
      "metadata": {
        "id": "i8LSEwU6tB6v"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5U0j4CutE-o",
        "outputId": "dd2332a3-82d5-4216-a214-fd3c390fcab4"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0000000000000002,\n",
              " 0.015579661038930867,\n",
              " 0.020008838615174385,\n",
              " 0.09210822317843292,\n",
              " 0.02540423054200211,\n",
              " 0.10481833227107434,\n",
              " 0.1831616205151224,\n",
              " 0.016687132346416188,\n",
              " 0.008580865542821586,\n",
              " 0.006602534475081826]"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_train = []"
      ],
      "metadata": {
        "id": "rDvugngTuEcX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in lines:\n",
        "  target_train.append(i[:i.find('\\t')])\n"
      ],
      "metadata": {
        "id": "1IVQBn4UtW2E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train"
      ],
      "metadata": {
        "id": "ftisfj7puusc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_test = []"
      ],
      "metadata": {
        "id": "gLoJH6h90Ad-"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in lines_2:\n",
        "  target_test.append(i[:i.find('\\t')])\n"
      ],
      "metadata": {
        "id": "g8GEaNqp0Jki"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_test"
      ],
      "metadata": {
        "id": "f54eAqT20PJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "X_train = dense_vectors\n",
        "y_train = target_train\n",
        "X_test = dense_vectors_2\n",
        "y_test = target_test"
      ],
      "metadata": {
        "id": "-WjNmBE5Ttrg"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HwMXQO1d8sq",
        "outputId": "371a15b8-3529-4c87-ffc4-cc8225d98c1b"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4oEVRm31K0m",
        "outputId": "b8c89ea6-e9da-4bfc-8453-c4683929a309"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:590: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 21s, sys: 193 ms, total: 1min 21s\n",
            "Wall time: 1min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, svc.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGQ1KTyK1T8V",
        "outputId": "20cf4afa-1ac7-46e9-e4b3-654aaaaafd91"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:590: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.956"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDClassifier()\n",
        "sgd.fit(X_train, y_train)\n",
        "accuracy_score(y_test, sgd.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThF_90c-1U1Z",
        "outputId": "4887678d-3ac6-463d-e961-ca740315ea07"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:590: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:590: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9873333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача №2"
      ],
      "metadata": {
        "id": "PIPxHTu22aVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "proc_words = [preprocess_text(text).split() for text in lines]\n",
        "embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
        "                 size=100,                 # embedding vector size\n",
        "                 min_count=3,             # consider words that occured at least 5 times\n",
        "                 window=3).wv"
      ],
      "metadata": {
        "id": "5OGkfjMj2c4x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sum(comment, embeddings):\n",
        "    \"\"\"\n",
        "    implement a function that converts preprocessed comment to a sum of token vectors\n",
        "    \"\"\"\n",
        "    embedding_dim = embeddings.vectors.shape[1]\n",
        "    features = np.zeros([embedding_dim], dtype='float32')\n",
        "\n",
        "    for word in preprocess_text(comment).split():\n",
        "        if word in embeddings:\n",
        "            features += embeddings[f'{word}']\n",
        "    \n",
        "    return features"
      ],
      "metadata": {
        "id": "OgafR7pO4Zik"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_trained.index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKYSwTLC4lm6",
        "outputId": "61e1c2ff-664b-4062-9206-9a7200130a10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15884"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "4Oik6oEq4xW0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in lines])\n",
        "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, target_train, test_size=0.2, random_state=0)\n",
        "X_train_wv.shape, X_test_wv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0OB7Xv04rXd",
        "outputId": "62f5c714-1491-415c-cd32-1fe100eb38c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2516, 100), (630, 100))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задча №3 "
      ],
      "metadata": {
        "id": "On3MYx657cxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "wv_model = clf.fit(X_train_wv, y_train)\n",
        "accuracy_score(y_test, wv_model.predict(X_test_wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPyV4fno5TRh",
        "outputId": "c92848c3-3988-40a2-fca2-ebbe52a93f9e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7476190476190476"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9equNEzVcbT"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "test.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}